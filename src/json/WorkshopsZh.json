{
  "categories": [
    {
      "name": "全部",
      "id": "all"
    },
    {
      "name": "北京智源-Triton 工作坊",
      "id": "baai-triton-workshop"
    },
    {
      "name": "仓颉 工作坊",
      "id": "cangjie-workshop"
    },
    {
      "name": "Dora 工作坊",
      "id": "dora-workshop"
    },
    {
      "name": "GDC / 开放钱包工作坊",
      "id": "gdc-openwallet-workshop"
    },
    {
      "name": "SGLang 工作坊",
      "id": "sglang-workshop"
    }
  ],
  "speakers": [
    {
      "id": "yin-zhang",
      "name": "张引",
      "bio": "张引是传统大学计算机科学（CS）和软件工程（SE）课程改革的积极促进者，致力于利用仓颉编程语言、现代应用开发框架、MVVM + IService架构模式、微服务参考架构和人工智能服务等丰富技术资源，将各种CS/SE课程相互关联，设计高度集成的CS/SE教育课程。他为技术社区贡献了“仓颉社区软件工程”等一系列免费的在线课程，展现出将尖端技术整合到传统大学的CS/SE课程中的潜力与优势。他还在东北大学软件学院组织了学生开源社区“仓颉兴趣组”，为仓颉社区贡献了一系列开源项目，并将这些开源项目引入到课程中，促进课程内容与开源社区的紧密融合。在未来的工作中，他会将持续集成、容器化、大语言模型等丰富的华为技术引入到课程中，帮助学生更好地理解现代开发技术。",
      "role": "副教授",
      "org": "东北大学",
      "roleOrg": "东北大学副教授",
      "image": "yin-zhang.png",
      "tag": "cangjie-workshop",
      "socialLinks": {
        "mastodon": "undefined",
        "twitter": "undefined",
        "github": "undefined",
        "linkedin": "undefined",
        "website": "undefined"
      },
      "status": "Accept",
      "draft": false,
      "title": "仓颉在高校开源软件工程教学实践分享",
      "abstract": "他还在东北大学软件学院组织了学生开源社区“仓颉兴趣组”，为仓颉社区贡献了一系列开源项目，并将这些开源项目引入到课程中，促进课程内容与开源社区的紧密融合",
      "language": "Chinese"
    },
    {
      "id": "jingrun-wu",
      "name": "吴京润",
      "bio": "北京初联科技有限公司 服务器应用架构师",
      "role": "服务器应用架构师",
      "org": "北京初联科技有限公司",
      "roleOrg": "北京初联科技有限公司服务器应用架构师",
      "image": "jingrun-wu.png",
      "tag": "cangjie-workshop",
      "socialLinks": {
        "mastodon": "undefined",
        "twitter": "undefined",
        "github": "undefined",
        "linkedin": "undefined",
        "website": "undefined"
      },
      "status": "Accept",
      "draft": false,
      "title": "基于仓颉的服务器应用套件：Fountain的开发实践",
      "abstract": "fountain：一个仓颉服务器应用开发套件 议题简介： 1. 深刻利用仓颉语言特性 2. 宏与注解有机结合 3. 简单的初始化方式 4. 各种服务器开发的工具API",
      "language": "Chinese"
    },
    {
      "id": "xuezhi-wang",
      "name": "王学智",
      "bio": "王学智，华为仓颉编程语言生态与产业发展总监",
      "role": "华为仓颉编程语言生态与产业发展总监",
      "org": "华为",
      "roleOrg": "华为仓颉编程语言生态与产业发展总监",
      "image": "xuezhi-wang.jpg",
      "tag": "cangjie-workshop",
      "socialLinks": {
        "mastodon": "undefined",
        "twitter": "undefined",
        "github": "undefined",
        "linkedin": "undefined",
        "website": "undefined"
      },
      "status": "Accept",
      "draft": false,
      "title": "仓颉编程语言生态建设进展介绍",
      "abstract": "整体仓颉编程语言整体生态进展",
      "language": "Chinese"
    },
    {
      "id": "shangming-cai",
      "name": "蔡尚明",
      "bio": "Shangming Cai received the Ph.D. degree in computer science from the Department of Computer Science and Technology, Tsinghua University, China, in 2022. He is currently an Engineer and Researcher with Alibaba Cloud Computing and an active contributor to open-source LLM projects such as SGLang, Mooncake, vLLM. His main research interests include distributed machine learning training, large language models, efficient serving systems, and big data analytics.",
      "role": "高级工程师",
      "org": "阿里云",
      "roleOrg": "阿里云高级工程师",
      "image": "shangming-cai.jpg",
      "tag": "sglang-workshop",
      "socialLinks": {
        "mastodon": "undefined",
        "twitter": "undefined",
        "github": "undefined",
        "linkedin": "undefined",
        "website": "undefined"
      },
      "status": "Accept",
      "draft": false,
      "title": "SGLang Prefill/Decode Disaggregation with Mooncake",
      "abstract": "Large Language Model (LLM) inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decoding (PD) Disaggregation in SGLang, which enables tailored optimizations for each. This presentation will introduce the implementation of Mooncake backend in detail, which is also the first integrated and default PD disaggregation backend of sglang. In addition to explaining the overall process, this talk will also detail how PD disaggregation works with features such as DP attention and MTP, and how to achieve different TP sizes for PD. Moreover, this talk will also introduce some of the work we have done in fault tolerance to ensure stable operation in a production environment.",
      "language": "English"
    }
  ]
}